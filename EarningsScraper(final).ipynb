{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from googlesearch import search\n",
    "except ImportError:\n",
    "    print(\"No module named google found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Nifty50Links.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=df['Company Name'].values\n",
    "symbols=df['NSESymbol'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandAlonelinks=[]\n",
    "\n",
    "for i in range(len(names)):\n",
    "    query=names[i]+\" moneycontrol quarterly results\"\n",
    "    for j in search(query,tld='co.in',num=1,pause=2):\n",
    "        StandAlonelinks.append(j)\n",
    "        print(j)\n",
    "        break\n",
    "        #As the first result shown in google is what we require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NetProfitScraper(i):\n",
    "    global soup\n",
    "    \n",
    "    page=requests.get(StandAlonelinks[i])\n",
    "    print('page created')\n",
    "    soup=bs(page.content,'lxml')\n",
    "    print('soup object created')\n",
    "    for link in soup.findAll('a',attrs={'href':re.compile(\"^.*(consolidated).*$\",re.IGNORECASE)}):\n",
    "        print('Returning link for',i)\n",
    "        return link\n",
    "\n",
    "Consolidatedlinks=[]\n",
    "\n",
    "for i in range(len(StandAlonelinks)):\n",
    "    Consolidatedlinks.append(NetProfitScraper(i))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=r'C:\\Users\\User\\Desktop\\Earnings Data files\\\\'\n",
    "os.mkdir(path)\n",
    "for i in names:\n",
    "    path=os.path.join(r'C:\\Users\\User\\Desktop\\Earnings Data files\\\\'+i)\n",
    "    os.mkdir(path)\n",
    "    print('Directory %s is created'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeData(i,count):\n",
    "    global quarters,pnl\n",
    "    \n",
    "    temp=i.split('?')\n",
    "    i=temp[0]+'/'+str(count)\n",
    "    #print(i)\n",
    "    \n",
    "    page=requests.get(i)\n",
    "    soup=bs(page.content,'lxml')\n",
    "    \n",
    "    \n",
    "    #print('soup object created')\n",
    "    \n",
    "    \n",
    "    \n",
    "    table=soup.find('table',attrs={'class':'mctable1'})\n",
    "    \n",
    "    table_rows=table.findAll('tr')\n",
    "    \n",
    "    \n",
    "    #print('Extracting data')\n",
    "    for tr in table_rows:\n",
    "        td=tr.findAll('td')\n",
    "        \n",
    "        try:\n",
    "            row=[tr.text for tr in td]\n",
    "                    \n",
    "            \n",
    "            if(re.compile(r'^Quarterly').match(row[0])):\n",
    "                for t in range(len(row)-1):\n",
    "                    quarters.append(row[t])\n",
    "                \n",
    "            elif(re.compile(r'^Net Profit').match(row[0])):\n",
    "                for t in range(len(row)-1):\n",
    "                    pnl.append(row[t])\n",
    "                \n",
    "        except(IndexError):\n",
    "            continue\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'C:\\Users\\User\\Desktop\\Earnings Data files\\\\'\n",
    "now=datetime.datetime.now()\n",
    "preY=now.year\n",
    "check=((preY-2012)*4)/5#Condition needed as we scrapped dates of quarterly results only upto 2012 \n",
    "\n",
    "error_list=[]\n",
    "#len(StandAlonelinks\n",
    "for i in range(len(StandAlonelinks)):\n",
    "    try:\n",
    "        link=StandAlonelinks[i]\n",
    "        df=pd.DataFrame(columns=['Quarters','PnL(in crores)'])\n",
    "        quarters=[]\n",
    "        \n",
    "        pnl=[]\n",
    "\n",
    "\n",
    "        counter=0\n",
    "        while(counter<check):\n",
    "            counter+=1\n",
    "            scrapeData(link,counter)\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(StandAlonelinks[i],' is done\\n',len(StandAlonelinks)-i-1,' :left')\n",
    "        df['Quarters']=quarters\n",
    "        df['PnL']=pnl\n",
    "\n",
    "        df=df.drop_duplicates(subset='Quarters',keep='first')\n",
    "        df=df.drop(0,axis=0)\n",
    "\n",
    "        df.to_csv(r''+path+names[i]+'\\\\data.csv')\n",
    "    except(AttributeError):\n",
    "        error_list.append(link)\n",
    "        print(StandAlonelinks[i],' has error\\n',len(StandAlonelinks)-i-1,' :left')\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contains list of all stocks with problems\n",
    "error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing data for consolidated stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path=r'C:\\Users\\User\\Desktop\\Earnings Data(consolidated) files\\\\'\n",
    "os.mkdir(path)\n",
    "for i in names:\n",
    "    path=os.path.join(r'C:\\Users\\User\\Desktop\\Earnings Data(consolidated) files\\\\'+i)\n",
    "    os.mkdir(path)\n",
    "    print('Directory %s is created'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r'C:\\Users\\User\\Desktop\\Earnings Data(consolidated) files\\\\'\n",
    "now=datetime.datetime.now()\n",
    "preY=now.year\n",
    "check=((preY-2012)*4)/5#Condition needed as we scrapped dates of quarterly results only upto 2012 \n",
    "\n",
    "error_list2=[]\n",
    "for i in range(len(Consolidatedlinks)):\n",
    "    try:\n",
    "        link=Consolidatedlinks[i]\n",
    "        df=pd.DataFrame(columns=['Quarters','PnL(in crores)'])\n",
    "        quarters=[]\n",
    "        pnl=[]\n",
    "\n",
    "\n",
    "        counter=0\n",
    "        while(counter<check):\n",
    "            counter+=1\n",
    "            scrapeData(link,counter)\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(Consolidatedlinks[i],' is done\\n',len(Consolidatedlinks)-i-1,' :left')\n",
    "        df['Quarters']=quarters\n",
    "        df['PnL']=pnl\n",
    "\n",
    "        df=df.drop_duplicates(subset='Quarters',keep='first')\n",
    "        df=df.drop(0,axis=0)\n",
    "\n",
    "        df.to_csv(r''+path+names[i]+'\\\\data.csv')\n",
    "    except(AttributeError):\n",
    "        error_list.append(link)\n",
    "        print(Consolidatedlinks[i],' has error\\n',len(Consolidatedlinks)-i-1,' :left')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contains list of all stocks with problems\n",
    "error_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date,datetime\n",
    "import os\n",
    "\n",
    "df=pd.read_csv('ind_nifty50list.csv',index_col=[0])#Please make sure you have an updated list of nifty50 stocks from NSE website\n",
    "\n",
    "\n",
    "\n",
    "nstocks=0#Will contain final number of stocks considered\n",
    "\n",
    "attr1={'class':'table-txt-right'}\n",
    "attr2={'class':'table table-BG-grey mT20'}#This is used to find the table containing Board meeting announcements\n",
    "\n",
    "finaldf=pd.read_csv('Nifty50Links.csv',index_col=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quarterCheck(tempD):\n",
    "    tempD=datetime.strptime(tempD,'%d-%m-%Y')\n",
    "    tempD=tempD.date()\n",
    "    \n",
    "    m=tempD.month\n",
    "    y=tempD.year\n",
    "    \n",
    "    if(m==1 or m==2 or m==3):\n",
    "        return('Dec \\''+str(y-1))\n",
    "    elif(m==4 or m==5 or m==6):\n",
    "        return('Mar \\''+str(y))\n",
    "    elif(m==7 or m==8 or m==9):\n",
    "        return('Jun \\''+str(y))\n",
    "    elif(m==10 or m==11 or m==12):\n",
    "        return('Sep \\''+str(y))\n",
    "    \n",
    "\n",
    "#This Function accepts link and data. Then it scrapes all the announcements made and searches for keywords if keyword\n",
    "# is found then it is classified accordingly. Then its date is recorded to see which month it belongs to and then calculates \n",
    "# the previous quarter using quarterCheck function\n",
    "\n",
    "def newsDeclare(templink2,name,path):\n",
    "    global errorlist\n",
    "    \n",
    "    \n",
    "    page=requests.get(templink2)\n",
    "    soup=bs(page.content,'lxml')\n",
    "    \n",
    "    \n",
    "    \n",
    "    table2=soup.find('div',attrs=attr2)\n",
    "    \n",
    "    table2rows=table2.find_all('tr')\n",
    "    \n",
    "    text=[]\n",
    "    \n",
    "    \n",
    "    for tr in table2rows:\n",
    "\n",
    "        td=tr.find_all('td')\n",
    "#        \n",
    "        temp=''\n",
    "        row=[tr.text for tr in td]\n",
    "\n",
    "        temp=tr.find('span',attrs={'title':re.compile(r'^.*(unaudited|audited)*.$',re.IGNORECASE)})\n",
    "\n",
    "        if(str(temp)!='None' and row[0]!='ANNOUNCEMENT DATE'):\n",
    "\n",
    "            text.append([row[0],quarterCheck(row[0]),'Audited Results Announcement'])\n",
    "\n",
    "        elif(row[0]!='ANNOUNCEMENT DATE'):\n",
    "\n",
    "            temp=tr.find('span',attrs={'title':re.compile(r'^.*quarterly*.$',re.IGNORECASE)})\n",
    "            if(str(temp)!='None'):\n",
    "                text.append([row[0],quarterCheck(row[0]),'Quarterly Results Announcement'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    tempdf=pd.DataFrame(text,columns=['Date of Announcement','Quarters','Announcement Type'])\n",
    "    tempdf=tempdf.drop_duplicates(subset='Quarters',keep='first')\n",
    "        #print(tempdf)\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        Edf=pd.read_csv(path+name+'\\data.csv',index_col=[0])\n",
    "\n",
    "        tempdf=tempdf.drop_duplicates('Quarters',keep='last')\n",
    "        Edf=Edf.drop('PnL(in crores)',axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        datesLen=len(tempdf)\n",
    "        Edf=Edf.drop(Edf.index[datesLen:],axis=0)\n",
    "        Edf['Date of Announcement']=tempdf['Date of Announcement'].values\n",
    "        Edf['Announcement Type']=tempdf['Announcement Type'].values\n",
    "\n",
    "\n",
    "        os.remove(path+name+'\\data.csv')\n",
    "        Edf.to_csv(path+name+'\\data.csv')\n",
    "        \n",
    "    except:\n",
    "        print(name,':Error')\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalLink=[]\n",
    "link=\"https://www.business-standard.com\"#This is the website to which extension is added\n",
    "link3=\"/corporate-action?purpose=board&fromDay=&fromMonth=&fromYear=&toDay=&toMonth=&toYear=\"#Used to get boardmeeting page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now finally have the list of websites for Nifty-50 stocks containing information regarding Board Meetings\n",
    "for i in range(len(finaldf)):\n",
    "    temp=(link+finaldf['Links'][i])\n",
    "    temp=temp.split('.html')\n",
    "    finalLink.append(temp[0]+link3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the final loop which helps create csvs containing data of announcements with their company names.\n",
    "\n",
    "\n",
    "for i in range(5,len(finalLink)):\n",
    "    path1=r'C:\\Users\\User\\Desktop\\Earnings Data files\\\\'#for standalone\n",
    "    path2=r'C:\\Users\\User\\Desktop\\Earnings Data(Consolidated) files\\\\'#for consolidated\n",
    "    newsDeclare(finalLink[i],finaldf['Company Name'][i],path1)\n",
    "    print('Companies left:',len(finalLink)-i-1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
