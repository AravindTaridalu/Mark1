{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re #Using regular expressions to search for special sequence of characters\n",
    "import winsound\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All arrays used in program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These 2 arrays are used to store the webpages extracted through webscraping\n",
    "IndustryLinks=np.empty(0)\n",
    "CompaniesLinks=np.empty(0)\n",
    "\n",
    "#This array stores the part of the name of industry used as keyword while parsing through html doc\n",
    "IndustryNames=np.empty(0)\n",
    "\n",
    "#These 2 arrays are used to store the names of industries and companies extracted\n",
    "CompanyNames=np.empty(0)\n",
    "AllIndustries=np.empty(0)\n",
    "\n",
    "#Used to store the values of parameters that is extracted for each company\n",
    "\n",
    "beta=np.empty(0)#contains beta values\n",
    "pe=np.empty(0)#contains p/e values\n",
    "div=np.empty(0)#contains dividend values\n",
    "indpe=np.empty(0)#contains industry p/e values\n",
    "pb=np.empty(0)#contains p/b values\n",
    "divy=np.empty(0)#contains dividend yield values\n",
    "deliv1d=np.empty(0)#contains 1-day deliverables\n",
    "deliv3d=np.empty(0)#contains 3-day deliverables\n",
    "deliv5d=np.empty(0)#contains 5-day deliverables\n",
    "deliv8d=np.empty(0)#contains 8-day deliverables\n",
    "\n",
    "df=pd.DataFrame(columns=['Industry','Company','Beta','P/E','Dividend(%)','Industry P/E','P/B','Dividend Yield(%)','Deliverables(1D)','Deliverables(3D)','Deliverables(5D)','Deliverables(8D)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebScraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Beta(Clink):\n",
    "    \n",
    "    page=requests.get(Clink)       #Using companylink passed tocreate soup object\n",
    "    soup=bs(page.content,'lxml')\n",
    "    \n",
    "    attr4={'class':'nsert','id':'div_nse_livebox_wrap'}           #Using attributes to locate nse section of \n",
    "    nseData=soup.find('div',attrs=attr4)                          #html doc\n",
    "    beta=nseData.find_all('div',attrs={'class':'disin vt'})       #Finding the part of the code \n",
    "    \n",
    "    for div in beta:\n",
    "        temp=str([div.text])\n",
    "        \n",
    "    temp1=temp.split('\\\\n')\n",
    "    #print(temp1[2])\n",
    "    try:\n",
    "        return float(temp1[2])\n",
    "    except:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terms(Clink):\n",
    "    try:\n",
    "        temp=np.empty(0)\n",
    "        temp1=np.empty(0)\n",
    "        page=requests.get(Clink)\n",
    "        soup=bs(page.content,'lxml')\n",
    "        attr={'class':'tab-pane fade','id':'consolidated_valuation','role':'tabpanel'}\n",
    "        div=soup.find('div',attrs=attr)\n",
    "        data=div.find_all('div',attrs={'class':'value_txtfr'})\n",
    "        for t in data:     \n",
    "            temp=np.append(temp,[t.text])\n",
    "        temp1=temp[11].split()\n",
    "        values=np.array([temp[1],temp[3],temp[5],temp[8],temp[9],temp1[9],temp1[4],temp1[6],temp1[8]])\n",
    "        series=[]\n",
    "        for i in values:\n",
    "            try:\n",
    "                series.append(float(i))\n",
    "            except(ValueError):\n",
    "                series.append(np.NaN)\n",
    "        return series\n",
    "    except:\n",
    "        series=[]\n",
    "        for i in range(0,9):\n",
    "            series.append(np.NaN)\n",
    "        return series\n",
    "        \n",
    "#pe,Div,IndPe,pb,DivY,deliv1D,deliv3D,deliv5D,deliv8D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get(\"https://www.moneycontrol.com/stocks/marketstats/industry-classification/nse/abrasives.html\")\n",
    "soup=bs(page.content,'lxml') #creating soup object to access links of all the industries\n",
    "\n",
    "attr={'class':'colLft'}\n",
    "section=soup.find('section',attrs=attr)#Scanning for section that contains list of all links\n",
    "\n",
    "\n",
    "attr2={'href':re.compile(\"/stocks/marketstats/industry-classification/nse/\")}\n",
    "links=section.find_all('a',attrs=attr2)#storing all href containing links after parsing isdone\n",
    "\n",
    "IndustryLinks=np.append(IndustryLinks,'https://www.moneycontrol.com/stocks/marketstats/industry-classification/nse/abrasives.html')\n",
    "IndustryNames=np.append(IndustryNames,'abrasives')\n",
    "link1='https://www.moneycontrol.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for link in links:\n",
    "    IndustryLinks=np.append(IndustryLinks,link1+str(link.get('href')))\n",
    "    temp1=IndustryLinks[i].split('nse/')\n",
    "    temp2=temp1[1].split('.html')\n",
    "    IndustryNames=np.append(IndustryNames,temp2[0])\n",
    "    i=i+1\n",
    "#Extracting all industry links from html doc and storing them in numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=0\n",
    "l=0\n",
    "for k in IndustryLinks:    \n",
    "    try:\n",
    "        page=requests.get(k)\n",
    "        soup=bs(page.content,'lxml')\n",
    "        t=IndustryNames[l].split('-')\n",
    "    \n",
    "        consent=True  #Purpose of consent is to check if that industry link leads to company links containing that industry name\n",
    "                  # for example in agriculture industry names of it dont appear in company link in that case consent is left\n",
    "                  # to be True so that the appropriate changes are made to parsing condition.\n",
    "    \n",
    "        for link in soup.findAll('a', attrs={'href': re.compile(\"https://www.moneycontrol.com/india/stockpricequote/\"+t[0])}):\n",
    "        \n",
    "            try:\n",
    "                CompaniesLinks=np.append(CompaniesLinks,link.get('href'))\n",
    "                temp=CompaniesLinks[i].split('/')\n",
    "                name=temp[6]+'('+temp[7]+')'\n",
    "                IndustryName=temp[5]\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "       #pe,Div,Indpe,pb,DivY,deliv1D,deliv3D,deliv5D,deliv8D\n",
    "            row=np.array(terms(CompaniesLinks[i]))\n",
    "        \n",
    "        #Gathering values of all parameters and storing them in their respective arrays corresponding to their companies and industries\n",
    "        \n",
    "            try:\n",
    "                beta=np.append(beta,Beta(CompaniesLinks[i]))\n",
    "            except:\n",
    "                beta=np.append(beta,np.NaN)\n",
    "            pe=np.append(pe,row[0])\n",
    "            div=np.append(div,row[1])\n",
    "            indpe=np.append(indpe,row[2])\n",
    "            pb=np.append(pb,row[3])\n",
    "            divy=np.append(divy,row[4])\n",
    "            deliv1d=np.append(deliv1d,row[5])\n",
    "            deliv3d=np.append(deliv3d,row[6])\n",
    "            deliv5d=np.append(deliv5d,row[7])\n",
    "            deliv8d=np.append(deliv8d,row[8])\n",
    "        \n",
    "            CompanyNames=np.append(CompanyNames,name)\n",
    "            AllIndustries=np.append(AllIndustries,IndustryName)\n",
    "        \n",
    "            i+=1\n",
    "            consent=False\n",
    "        \n",
    "    \n",
    "        if(consent):\n",
    "        \n",
    "            page=requests.get(k)\n",
    "            soup=bs(page.content,'lxml')\n",
    "        \n",
    "            span=soup.find_all('span',attrs={'class':'gld13 disin'})\n",
    "        \n",
    "            a=[span]\n",
    "            t=str(a[0]).split('a href=\"')\n",
    "        \n",
    "            for i in range(len(t)):\n",
    "                #Extracting link ofcompany from html doc by splitting data\n",
    "                if(i%2==1):\n",
    "                    a=t[i].split('\"')\n",
    "                    for j in a:\n",
    "                        if(str('https://www.moneycontrol.com/india/stockpricequote/') in str(j)):\n",
    "                        \n",
    "                            CompaniesLinks=np.append(CompaniesLinks,j)\n",
    "                            temp=CompaniesLinks[i].split('/')\n",
    "                            name=temp[6]+'('+temp[7]+')'\n",
    "                            IndustryName=temp[5]                            \n",
    "                        \n",
    "                        \n",
    "                        #Gathering values of all parameters and storing them in their respective arrays corresponding to their companies and industries\n",
    "                            try:\n",
    "                                beta=np.append(beta,Beta(CompaniesLinks[i]))\n",
    "                            except:\n",
    "                                beta=np.append(beta,np.NaN)\n",
    "                            \n",
    "                            pe=np.append(pe,row[0])\n",
    "                            div=np.append(div,row[1])\n",
    "                            indpe=np.append(indpe,row[2])\n",
    "                            pb=np.append(pb,row[3])\n",
    "                            divy=np.append(divy,row[4])\n",
    "                            deliv1d=np.append(deliv1d,row[5])\n",
    "                            deliv3d=np.append(deliv3d,row[6])\n",
    "                            deliv5d=np.append(deliv5d,row[7])\n",
    "                            deliv8d=np.append(deliv8d,row[8])\n",
    "        \n",
    "                            CompanyNames=np.append(CompanyNames,name)\n",
    "                            AllIndustries=np.append(AllIndustries,IndustryName)\n",
    "                        i+=1\n",
    "        l+=1\n",
    "        print(\"Number of industries left:\",(len(IndustryLinks)-l))\n",
    "    except:\n",
    "        print('Error in Industry:',j)\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Industry']=AllIndustries\n",
    "df['Company']=CompanyNames\n",
    "df['Beta']=beta\n",
    "df['P/E']=pe\n",
    "df['Dividend(%)']=div\n",
    "df['Industry P/E']=indpe\n",
    "df['P/B']=pb\n",
    "df['Dividend Yield(%)']=divy\n",
    "df['Deliverables(1D)']=deliv1d\n",
    "df['Deliverables(3D)']=deliv3d\n",
    "df['Deliverables(5D)']=deliv5d\n",
    "df['Deliverables(8D)']=deliv8d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop_duplicates(subset='Company',keep='first')#Removes all duplicates based on company name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
